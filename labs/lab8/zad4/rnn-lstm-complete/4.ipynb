{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "[[[0.59768957 0.29951566 0.6383525  ... 0.35171047 0.4621021  0.1296392 ]\n",
      "  [0.27627876 0.73250085 0.17932197 ... 0.4253616  0.15426941 0.5463728 ]\n",
      "  [0.5288734  0.2864565  0.52885914 ... 0.1372107  0.45171747 0.18274526]\n",
      "  ...\n",
      "  [0.9393706  0.19022822 0.81109643 ... 0.95097214 0.39572543 0.76849914]\n",
      "  [0.5673457  0.9824833  0.57718766 ... 0.07239109 0.7474129  0.6447374 ]\n",
      "  [0.17018914 0.28966182 0.61178386 ... 0.8636416  0.43739802 0.5905932 ]]\n",
      "\n",
      " [[0.5316816  0.34491494 0.1711641  ... 0.5551112  0.6159994  0.321305  ]\n",
      "  [0.8590781  0.8880267  0.1096105  ... 0.06226259 0.14322577 0.29996225]\n",
      "  [0.02353977 0.8960547  0.6607008  ... 0.61787075 0.6665709  0.8597538 ]\n",
      "  ...\n",
      "  [0.836847   0.3775269  0.13083369 ... 0.9873993  0.96212023 0.6571642 ]\n",
      "  [0.99890214 0.359495   0.3711932  ... 0.17397846 0.97291553 0.25700295]\n",
      "  [0.86901885 0.87381965 0.71815544 ... 0.42315033 0.949224   0.6394596 ]]\n",
      "\n",
      " [[0.91839224 0.80339235 0.10230991 ... 0.15551412 0.65036607 0.07126582]\n",
      "  [0.41910937 0.38182214 0.69085556 ... 0.7173883  0.85178244 0.14058574]\n",
      "  [0.44252744 0.3069443  0.6681134  ... 0.7585377  0.07808016 0.20053777]\n",
      "  ...\n",
      "  [0.8817313  0.0028186  0.7230011  ... 0.97859234 0.5933656  0.11963832]\n",
      "  [0.35125732 0.66800004 0.39973074 ... 0.5167726  0.25546548 0.47075108]\n",
      "  [0.87719506 0.34864727 0.60120595 ... 0.07674229 0.82768327 0.53069663]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.73800284 0.5367284  0.8039994  ... 0.8004548  0.27961004 0.93144935]\n",
      "  [0.8817591  0.09324643 0.81785923 ... 0.04402095 0.41918725 0.532665  ]\n",
      "  [0.9387938  0.83395845 0.8510061  ... 0.69848466 0.52562106 0.3526304 ]\n",
      "  ...\n",
      "  [0.9074536  0.7931372  0.16760498 ... 0.26068094 0.45510164 0.90500695]\n",
      "  [0.4693628  0.67379725 0.39661047 ... 0.50539213 0.53931355 0.5893603 ]\n",
      "  [0.75705534 0.4076524  0.19164518 ... 0.64572453 0.02850413 0.02285016]]\n",
      "\n",
      " [[0.42989704 0.9608247  0.0460281  ... 0.2942031  0.7388706  0.3077656 ]\n",
      "  [0.6606375  0.36212122 0.75031036 ... 0.30221447 0.19935326 0.6060735 ]\n",
      "  [0.41733298 0.49314052 0.43422726 ... 0.6564225  0.56906873 0.3983513 ]\n",
      "  ...\n",
      "  [0.371514   0.50812805 0.93174857 ... 0.5970032  0.8290955  0.6864391 ]\n",
      "  [0.23895428 0.50274485 0.74303627 ... 0.6100504  0.9960779  0.7808088 ]\n",
      "  [0.5796083  0.9280738  0.10348061 ... 0.8535259  0.7473956  0.976072  ]]\n",
      "\n",
      " [[0.5651267  0.58198917 0.7064122  ... 0.63812345 0.8153052  0.6240304 ]\n",
      "  [0.00264236 0.31625736 0.01024955 ... 0.79354817 0.49613118 0.2501243 ]\n",
      "  [0.8421781  0.37051597 0.36006117 ... 0.03055438 0.1833293  0.27130267]\n",
      "  ...\n",
      "  [0.18266794 0.21968144 0.98654723 ... 0.7198999  0.5698576  0.99350786]\n",
      "  [0.48023582 0.33741564 0.7156713  ... 0.10888448 0.65166557 0.13176316]\n",
      "  [0.74394184 0.02151516 0.2088846  ... 0.64135957 0.0833682  0.00502324]]]\n",
      "Output: \n",
      "tf.Tensor(\n",
      "[[-0.2388485  -0.49985668  0.77007127 -0.3791792 ]\n",
      " [-0.8232568  -0.7115113  -0.2191614  -0.63291645]\n",
      " [-0.20177127 -0.12544543  0.7612897  -0.6077861 ]\n",
      " [ 0.16956446  0.17314143  0.44763923 -0.5475146 ]\n",
      " [-0.5069034   0.5941422  -0.02173123 -0.09785744]\n",
      " [-0.9030227  -0.9331477  -0.07718529 -0.5109089 ]\n",
      " [-0.5451918  -0.02772014  0.5636609  -0.73234206]\n",
      " [-0.72179645  0.5049697   0.08427449 -0.5111208 ]\n",
      " [-0.55519325  0.61442035  0.7663337  -0.77451676]\n",
      " [-0.76540136  0.5587768   0.26046568 -0.9368747 ]\n",
      " [-0.4661798  -0.01188005  0.40587965 -0.32299384]\n",
      " [-0.19292116  0.09410884  0.04494682 -0.8025864 ]\n",
      " [-0.8697045  -0.45395434  0.12157212  0.039056  ]\n",
      " [-0.5531879   0.4008219   0.7015819  -0.33772364]\n",
      " [-0.02950083  0.74213153 -0.0125159   0.10707694]\n",
      " [ 0.02034872  0.8656813  -0.00684    -0.5677376 ]\n",
      " [-0.4729085  -0.23602043  0.83480114 -0.6271636 ]\n",
      " [-0.4775208   0.22951563 -0.09341154 -0.52732587]\n",
      " [-0.00197035  0.9234953   0.4785666  -0.82142943]\n",
      " [-0.8997312   0.7155615   0.22641666 -0.88133156]\n",
      " [-0.56351924  0.5810327   0.41439778 -0.51438665]\n",
      " [-0.1686714   0.12475883  0.7759738  -0.53300023]\n",
      " [-0.8665504   0.65538317 -0.05713691 -0.79917014]\n",
      " [-0.86304027 -0.47230726  0.12946664 -0.479358  ]\n",
      " [-0.7849725   0.38086313  0.01144443 -0.28779146]\n",
      " [-0.8016592   0.564731    0.02867254 -0.9532178 ]\n",
      " [ 0.19655426  0.8002795   0.03254557 -0.75815994]\n",
      " [-0.2999838   0.37320396  0.32752436  0.3610741 ]\n",
      " [ 0.27336997  0.11147806 -0.15099934 -0.6153369 ]\n",
      " [ 0.35076973  0.31706032  0.6510883  -0.44380495]\n",
      " [-0.22421345  0.42957196 -0.12384487 -0.7017691 ]\n",
      " [-0.8044617   0.05477939  0.78163475 -0.7724191 ]], shape=(32, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "inputs = np.random.random([32, 10, 8]).astype(np.float32)\n",
    "print(\"Inputs: \")\n",
    "print(inputs)\n",
    "\n",
    "simple_rnn = SimpleRNN(4)\n",
    "\n",
    "output = simple_rnn(inputs)  # The output has shape `[32, 4]`.\n",
    "print(\"Output: \")\n",
    "print(output)\n",
    "\n",
    "simple_rnn = SimpleRNN(\n",
    "    4, return_sequences=True, return_state=True)\n",
    "\n",
    "# whole_sequence_output has shape `[32, 10, 4]`.\n",
    "# final_state has shape `[32, 4]`.\n",
    "whole_sequence_output, final_state = simple_rnn(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wx =  [[ 0.83494985 -1.0237857 ]]  wh =  [[ 0.9630548  -0.26930588]\n",
      " [ 0.26930588  0.9630547 ]]  bh =  [0. 0.]  wy = [[-0.79898554]\n",
      " [ 0.46358728]] by =  [0.]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "h1 =  [[ 0.83494985 -1.02378571]] h2 =  [[ 2.19829063 -3.25838999]] h3 =  [[ 3.74442024 -6.80137758]]\n",
      "Prediction from network  [[-6.144769]]\n",
      "Prediction from our computation  [[-6.14476979]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Piotr\\Desktop\\School\\IntelOb\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "\n",
    "\n",
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, \n",
    "                        activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "demo_model = create_RNN(2, 1, (3,1), activation=['linear', 'linear'])\n",
    "\n",
    "wx = demo_model.get_weights()[0]\n",
    "wh = demo_model.get_weights()[1]\n",
    "bh = demo_model.get_weights()[2]\n",
    "wy = demo_model.get_weights()[3]\n",
    "by = demo_model.get_weights()[4]\n",
    "\n",
    "print('wx = ', wx, ' wh = ', wh, ' bh = ', bh, ' wy =', wy, 'by = ', by)\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "# Reshape the input to the required sample_size x time_steps x features \n",
    "x_input = np.reshape(x,(1, 3, 1))\n",
    "y_pred_model = demo_model.predict(x_input)\n",
    "\n",
    "\n",
    "m = 2\n",
    "h0 = np.zeros(m)\n",
    "h1 = np.dot(x[0], wx) + h0 + bh\n",
    "h2 = np.dot(x[1], wx) + np.dot(h1,wh) + bh\n",
    "h3 = np.dot(x[2], wx) + np.dot(h2,wh) + bh\n",
    "o3 = np.dot(h3, wy) + by\n",
    "\n",
    "print('h1 = ', h1,'h2 = ', h2,'h3 = ', h3)\n",
    "\n",
    "print(\"Prediction from network \", y_pred_model)\n",
    "print(\"Prediction from our computation \", o3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Piotr\\Desktop\\School\\IntelOb\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 - 1s - 5ms/step - loss: 0.0520\n",
      "Epoch 2/20\n",
      "187/187 - 0s - 826us/step - loss: 0.0293\n",
      "Epoch 3/20\n",
      "187/187 - 0s - 794us/step - loss: 0.0244\n",
      "Epoch 4/20\n",
      "187/187 - 0s - 771us/step - loss: 0.0214\n",
      "Epoch 5/20\n",
      "187/187 - 0s - 781us/step - loss: 0.0192\n",
      "Epoch 6/20\n",
      "187/187 - 0s - 760us/step - loss: 0.0167\n",
      "Epoch 7/20\n",
      "187/187 - 0s - 784us/step - loss: 0.0149\n",
      "Epoch 8/20\n",
      "187/187 - 0s - 781us/step - loss: 0.0141\n",
      "Epoch 9/20\n",
      "187/187 - 0s - 772us/step - loss: 0.0134\n",
      "Epoch 10/20\n",
      "187/187 - 0s - 786us/step - loss: 0.0124\n",
      "Epoch 11/20\n",
      "187/187 - 0s - 757us/step - loss: 0.0118\n",
      "Epoch 12/20\n",
      "187/187 - 0s - 795us/step - loss: 0.0115\n",
      "Epoch 13/20\n",
      "187/187 - 0s - 776us/step - loss: 0.0110\n",
      "Epoch 14/20\n",
      "187/187 - 0s - 800us/step - loss: 0.0103\n",
      "Epoch 15/20\n",
      "187/187 - 0s - 770us/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "187/187 - 0s - 751us/step - loss: 0.0095\n",
      "Epoch 17/20\n",
      "187/187 - 0s - 739us/step - loss: 0.0093\n",
      "Epoch 18/20\n",
      "187/187 - 0s - 759us/step - loss: 0.0093\n",
      "Epoch 19/20\n",
      "187/187 - 0s - 760us/step - loss: 0.0087\n",
      "Epoch 20/20\n",
      "187/187 - 0s - 751us/step - loss: 0.0084\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Train RMSE: 0.095 RMSE\n",
      "Test RMSE: 0.124 RMSE\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameter split_percent defines the ratio of training examples\n",
    "def get_train_test(url, split_percent=0.8):\n",
    "    df = read_csv(url, usecols=[1], engine='python')\n",
    "    data = np.array(df.values.astype('float32'))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data = scaler.fit_transform(data).flatten()\n",
    "    n = len(data)\n",
    "    # Point for splitting data into train and test\n",
    "    split = int(n*split_percent)\n",
    "    train_data = data[range(split)]\n",
    "    test_data = data[split:]\n",
    "    return train_data, test_data, data\n",
    " \n",
    "# Prepare the input X and target Y\n",
    "def get_XY(dat, time_steps):\n",
    "    Y_ind = np.arange(time_steps, len(dat), time_steps)\n",
    "    Y = dat[Y_ind]\n",
    "    rows_x = len(Y)\n",
    "    X = dat[range(time_steps*rows_x)]\n",
    "    X = np.reshape(X, (rows_x, time_steps, 1))    \n",
    "    return X, Y\n",
    " \n",
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    " \n",
    "def print_error(trainY, testY, train_predict, test_predict):    \n",
    "    # Error of predictions\n",
    "    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n",
    "    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n",
    "    # Print RMSE\n",
    "    print('Train RMSE: %.3f RMSE' % (train_rmse))\n",
    "    print('Test RMSE: %.3f RMSE' % (test_rmse))    \n",
    " \n",
    "# Plot the result\n",
    "def plot_result(trainY, testY, train_predict, test_predict):\n",
    "    actual = np.append(trainY, testY)\n",
    "    predictions = np.append(train_predict, test_predict)\n",
    "    rows = len(actual)\n",
    "    plt.figure(figsize=(15, 6), dpi=80)\n",
    "    plt.plot(range(rows), actual)\n",
    "    plt.plot(range(rows), predictions)\n",
    "    plt.axvline(x=len(trainY), color='r')\n",
    "    plt.legend(['Actual', 'Predictions'])\n",
    "    plt.xlabel('Observation number after given time steps')\n",
    "    plt.ylabel('Sunspots scaled')\n",
    "    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\n",
    "    filename = 'rnn_plot.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "sunspots_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv'\n",
    "time_steps = 12\n",
    "train_data, test_data, data = get_train_test(sunspots_url)\n",
    "trainX, trainY = get_XY(train_data, time_steps)\n",
    "testX, testY = get_XY(test_data, time_steps)\n",
    " \n",
    "# Create model and train\n",
    "model = create_RNN(hidden_units=3, dense_units=1, \n",
    "                   input_shape=(time_steps,1), \n",
    "                   activation=['tanh', 'tanh'])\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)\n",
    " \n",
    "# make predictions\n",
    "train_predict = model.predict(trainX)\n",
    "test_predict = model.predict(testX)\n",
    " \n",
    "# Print error\n",
    "print_error(trainY, testY, train_predict, test_predict)\n",
    " \n",
    "#Plot result\n",
    "plot_result(trainY, testY, train_predict, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm03 first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Piotr\\Desktop\\School\\IntelOb\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Total Characters:  164016\n",
      "Total Vocab:  64\n",
      "Total Patterns:  163916\n",
      "WARNING:tensorflow:From c:\\Users\\Piotr\\Desktop\\School\\IntelOb\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Piotr\\Desktop\\School\\IntelOb\\.venv\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Seed:\n",
      "\" een,” and she hurried out of the room. the cook\n",
      "threw a frying-pan after her as she went out, but it \"\n",
      "e toe toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toete the toet\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-05-2.6608.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm03 second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  164016\n",
      "Total Vocab:  64\n",
      "Total Patterns:  163916\n",
      "Seed:\n",
      "\" nd\n",
      "confusion, as the large birds complained that they could not taste\n",
      "theirs, and the small ones cho \"\n",
      " the tas oo the toaee an the cade, and the was sol tiing the was so tee toeee an the cad so tee that she was so toee at the cadl, and the wes soi tiied the was aol to the tooee an the cad to tee thet she was aolie  the wooed the was aol to the tooee an the cad to tee thet she was aol to the tooee an the cad to the toiee  “he tou dn wou thin i saod tou ”hu aalit i saed to the toue tf the tooee ”huh the boe oo the tooee ”huh the boe to the tooee ”huh the boe to the tooee ”huh the boe aoo aoe toee \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-10-2.2431.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm05 first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:  38030\n",
      "Unique Tokens (Token Vocab):  3236\n",
      "Total Patterns:  37930\n",
      "Seed:\n",
      "\" besides , _she ’ s_ she , and _i ’ m_ i , and — oh dear , how puzzling it all is ! i ’ ll try if i know all the things i used to know . let me see : four times five is twelve , and four times six is thirteen , and four times seven is — oh dear ! i shall never get to twenty at that rate ! however , the multiplication table doesn ’ t signify : let ’ s try geography . london is the capital of paris , and paris \"\n",
      "Generated text:\n",
      "the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the , “ the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "tokenized_text = wordpunct_tokenize(raw_text)\n",
    "tokens = sorted(list(dict.fromkeys(tokenized_text)))\n",
    "\n",
    "#print(\"Tokens: \")\n",
    "#print(tokens)\n",
    "tok_to_int = dict((c, i) for i, c in enumerate(tokens))\n",
    "int_to_tok = dict((i, c) for i, c in enumerate(tokens))\n",
    "#print(\"TokensToNumbers: \")\n",
    "#print(tok_to_int)\n",
    "\n",
    "# summarize the loaded data\n",
    "n_tokens = len(tokenized_text)\n",
    "n_token_vocab = len(tokens)\n",
    "print(\"Total Tokens: \", n_tokens)\n",
    "print(\"Unique Tokens (Token Vocab): \", n_token_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_tokens - seq_length, 1):\n",
    "\tseq_in = tokenized_text[i:i + seq_length]\n",
    "\tseq_out = tokenized_text[i + seq_length]\n",
    "\tdataX.append([tok_to_int[tok] for tok in seq_in])\n",
    "\tdataY.append(tok_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_token_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"big-token-model-05-5.8765.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_tok[value] for value in pattern]), \"\\\"\")\n",
    "# generate tokens\n",
    "print(\"Generated text:\")\n",
    "for i in range(100):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_token_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_tok[index]\n",
    "\tseq_in = [int_to_tok[value] for value in pattern]\n",
    "\tsys.stdout.write(result+\" \")\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm05 second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:  38030\n",
      "Unique Tokens (Token Vocab):  3236\n",
      "Total Patterns:  37930\n",
      "Seed:\n",
      "\" me your history , you know ,” said alice , “ and why it is you hate — c and d ,” she added in a whisper , half afraid that it would be offended again . “ mine is a long and a sad tale !” said the mouse , turning to alice , and sighing . “ it _is_ a long tail , certainly ,” said alice , looking down with wonder at the mouse ’ s tail ; “ but why do you call it sad ?” and she kept on puzzling about it while the mouse \"\n",
      "Generated text:\n",
      ", “ the , “ the , “ the , said the , said the , said the ’ s , “ the , “ t ’ s , said the , “ t ’ s , “ the , “ t ’ s , said the , “ t ’ s , “ the , “ t , said ’ t , “ i , “ t ’ s , said the , “ t ’ s , said the , “ t ’ s , “ the , “ t ’ s , said the , “ t \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "tokenized_text = wordpunct_tokenize(raw_text)\n",
    "tokens = sorted(list(dict.fromkeys(tokenized_text)))\n",
    "\n",
    "#print(\"Tokens: \")\n",
    "#print(tokens)\n",
    "tok_to_int = dict((c, i) for i, c in enumerate(tokens))\n",
    "int_to_tok = dict((i, c) for i, c in enumerate(tokens))\n",
    "#print(\"TokensToNumbers: \")\n",
    "#print(tok_to_int)\n",
    "\n",
    "# summarize the loaded data\n",
    "n_tokens = len(tokenized_text)\n",
    "n_token_vocab = len(tokens)\n",
    "print(\"Total Tokens: \", n_tokens)\n",
    "print(\"Unique Tokens (Token Vocab): \", n_token_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_tokens - seq_length, 1):\n",
    "\tseq_in = tokenized_text[i:i + seq_length]\n",
    "\tseq_out = tokenized_text[i + seq_length]\n",
    "\tdataX.append([tok_to_int[tok] for tok in seq_in])\n",
    "\tdataY.append(tok_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_token_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"big-token-model-10-5.2616.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_tok[value] for value in pattern]), \"\\\"\")\n",
    "# generate tokens\n",
    "print(\"Generated text:\")\n",
    "for i in range(100):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_token_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_tok[index]\n",
    "\tseq_in = [int_to_tok[value] for value in pattern]\n",
    "\tsys.stdout.write(result+\" \")\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
